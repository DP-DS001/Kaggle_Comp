{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Walkthrough\n",
    "\n",
    "In this notebook, we will go through the steps of training and deploying a model using AWS Sagemaker.\n",
    "\n",
    "We will be using the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) module throughout this notebook. \n",
    "\n",
    "We will use two different types of models for our predictions, a model built using scikit-learn, and one using an implementation of XGBoost available from sagemaker, including performing hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initial Setup\n",
    "\n",
    "First, let's bring our data over. It is the familiar King County Housing dataset, and is currently sitting in an s3 bucket with path *s3://nss-ds3/datasets/kc_house_data.csv*\n",
    "\n",
    "Let's use the *boto3* library to fetch our data. We'll do some tricks and read it directly to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj1 = s3.get_object(Bucket='nss-ds3', Key='datasets/kaggle/train.csv')\n",
    "obj2 = s3.get_object(Bucket='nss-ds3', Key='datasets/kaggle/test.csv')\n",
    "train = pd.read_csv(io.BytesIO(obj1['Body'].read()))\n",
    "kaggle_test = pd.read_csv(io.BytesIO(obj2['Body'].read()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train[['time', 'signal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we need to set create a sagemaker session and get our execution role. We will need to pass these as arguments when we create our models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'sagemaker_kaggle_dalila'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Modeling\n",
    "\n",
    "We'll do some minor preprocessing of our data and then export the results to a csv. Notice that we will put the column we want to predict at the front, because this is what the XGBoost model will expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['open_channels','time','signal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(train, test_size = 0.2)\n",
    "\n",
    "#X = train[['time', 'signal']]\n",
    "#y = train[['open_channels']]\n",
    "#X_train, y_train, X_test, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "train.to_csv('data/kaggs_train.csv', index = False)\n",
    "\n",
    "#X.to_csv('data/kaggs_train_X.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to put our training data into the s3 bucket for our sagemaker instance. The following cell will take the contents of the data folder (currently our train.csv) and copy them over to a folder sm-housing/data in s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'sm-kaggle'\n",
    "\n",
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "train_input = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to fit our scikit-learn model. For this to work, we need to upload the training script to our notebook instance, as the SKLearn class will be looking for it to use as an entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'housing_script_rf.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit our model, telling it where to look for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-22 16:48:18 Starting - Starting the training job...\n",
      "2020-03-22 16:48:21 Starting - Launching requested ML instances...\n",
      "2020-03-22 16:49:15 Starting - Preparing the instances for training......\n",
      "2020-03-22 16:49:58 Downloading - Downloading input data...\n",
      "2020-03-22 16:50:41 Training - Training image download completed. Training in progress..\u001b[34m2020-03-22 16:50:42,643 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:42,645 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:42,655 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:42,932 sagemaker-containers INFO     Module housing_script_rf does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:42,932 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:42,932 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:42,933 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: housing-script-rf\n",
      "  Building wheel for housing-script-rf (setup.py): started\n",
      "  Building wheel for housing-script-rf (setup.py): finished with status 'done'\n",
      "  Created wheel for housing-script-rf: filename=housing_script_rf-1.0.0-py2.py3-none-any.whl size=6230 sha256=83716507b304c1f4898b6ffabb688aaa112ec49836dae402515b89aa7ab72446\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cxzx1mz7/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built housing-script-rf\u001b[0m\n",
      "\u001b[34mInstalling collected packages: housing-script-rf\u001b[0m\n",
      "\u001b[34mSuccessfully installed housing-script-rf-1.0.0\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:44,356 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-22 16:50:44,366 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-03-22-16-48-18-491\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-339692866702/sagemaker-scikit-learn-2020-03-22-16-48-18-491/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"housing_script_rf\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"housing_script_rf.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=housing_script_rf.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=housing_script_rf\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-339692866702/sagemaker-scikit-learn-2020-03-22-16-48-18-491/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-03-22-16-48-18-491\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-339692866702/sagemaker-scikit-learn-2020-03-22-16-48-18-491/source/sourcedir.tar.gz\",\"module_name\":\"housing_script_rf\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"housing_script_rf.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m housing_script_rf\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m2020-03-22 17:04:09,597 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-22 17:04:20 Uploading - Uploading generated training model\n",
      "2020-03-22 17:04:20 Completed - Training job completed\n",
      "Training seconds: 862\n",
      "Billable seconds: 862\n"
     ]
    }
   ],
   "source": [
    "sklearn.fit({'train': train_input})\n",
    "#sklearn.fit({'kaggs_train_X': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, we have two options. First, we can retrieve the actual model (which can also be downloaded to your local machine). Second, we can deploy the model. For our scikit-learn model, will go with the first option. We'll look at the second one for the xgboost model that we'll do next.\n",
    "\n",
    "Let's see where the model output is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-339692866702/sagemaker-scikit-learn-2020-03-22-16-48-18-491/output/model.tar.gz'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').describe_training_job(\n",
    "    TrainingJobName=sklearn.latest_training_job.job_name)['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sub in what you see above and then uncomment the following lines\n",
    "s3.download_file(Bucket = 'sagemaker-us-east-2-339692866702', \n",
    "                 Key = 'sagemaker-scikit-learn-2020-03-22-16-48-18-491/output/model.tar.gz',\n",
    "                 Filename = 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tar = tarfile.open('model.tar.gz')\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/base.py:253: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.20.0 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/base.py:253: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.20.0 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "rf_model = joblib.load('model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it does on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(test.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the mean absolute error tells us how far off our predictions are, on average, from the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5994199083732523"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(test.iloc[:,0], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0001</td>\n",
       "      <td>-2.6498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0002</td>\n",
       "      <td>-2.8494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.0003</td>\n",
       "      <td>-2.8600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.0004</td>\n",
       "      <td>-2.4350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0005</td>\n",
       "      <td>-2.6155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  signal\n",
       "0  500.0001 -2.6498\n",
       "1  500.0002 -2.8494\n",
       "2  500.0003 -2.8600\n",
       "3  500.0004 -2.4350\n",
       "4  500.0005 -2.6155"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaggle_test = kaggle_test.values.reshape(-1,1)\n",
    "kaggle_pred = rf_model.predict(kaggle_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_output = kaggle_pred.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj3 = s3.get_object(Bucket='nss-ds3', Key='datasets/kaggle/sample_submission.csv')\n",
    "sample_sub = pd.read_csv(io.BytesIO(obj3['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.time = sample_sub.time.apply(lambda x: '{:.4f}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.open_channels = kaggle_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv('kaggle_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Using sagemaker's XGBoost Algorithm\n",
    "\n",
    "Now, we will see how to use the XGBoost algorithm.\n",
    "\n",
    "We need to get the location of the xgboost model. This is done with the get_image_uri method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name    \n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()   \n",
    "prefix = 'kaggle_xgb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(region, 'xgboost', repo_version='0.90-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create an Estimator instance, pointing to the xgboost container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyperparameters for our model. A list of available hyperparameters and what they represent is available here:https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(\n",
    "    num_round=100,\n",
    "    rate_drop=0.3,\n",
    "    alpha = 0.25,\n",
    "    lambda = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move our training data to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('data/train.csv')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-14 00:43:29 Starting - Starting the training job...\n",
      "2020-03-14 00:43:32 Starting - Launching requested ML instances......\n",
      "2020-03-14 00:44:36 Starting - Preparing the instances for training......\n",
      "2020-03-14 00:45:38 Downloading - Downloading input data...\n",
      "2020-03-14 00:46:30 Training - Training image download completed. Training in progress...\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[00:46:33] 17291x85 matrix with 1469735 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 17291 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:471337\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:349028\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:265043\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:207124\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:170212\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:144285\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:128699\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:118886\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:111866\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:107132\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:103872\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:101630\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:99576.1\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:97398.9\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:96156.5\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:95089.2\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:93634.5\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:92691.5\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:91880.1\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:90900.6\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:90002.4\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:89352.4\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:88789\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:88245.1\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:87594.9\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:86776.5\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:86281.7\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:85815.5\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:84943.7\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:84215.5\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:83930.7\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:83365.1\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:83099.1\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:82732.1\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:81605.2\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:81016.5\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:80434.8\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:80068.8\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:79610.3\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:79375.1\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:79001.5\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:78515.6\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:78143.7\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:77582.4\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:77365.6\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:76725.7\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:76279.7\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:76171.2\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:75831.9\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:75287.6\u001b[0m\n",
      "\u001b[34m[50]#011train-rmse:75068\u001b[0m\n",
      "\u001b[34m[51]#011train-rmse:74825.6\u001b[0m\n",
      "\u001b[34m[52]#011train-rmse:74645.2\u001b[0m\n",
      "\u001b[34m[53]#011train-rmse:74334.5\u001b[0m\n",
      "\u001b[34m[54]#011train-rmse:74157.7\u001b[0m\n",
      "\u001b[34m[55]#011train-rmse:73886.7\u001b[0m\n",
      "\u001b[34m[56]#011train-rmse:73582.9\u001b[0m\n",
      "\u001b[34m[57]#011train-rmse:73310.8\u001b[0m\n",
      "\u001b[34m[58]#011train-rmse:73136.2\u001b[0m\n",
      "\u001b[34m[59]#011train-rmse:72824.2\u001b[0m\n",
      "\u001b[34m[60]#011train-rmse:72604.4\u001b[0m\n",
      "\u001b[34m[61]#011train-rmse:72454.9\u001b[0m\n",
      "\u001b[34m[62]#011train-rmse:72104.8\u001b[0m\n",
      "\u001b[34m[63]#011train-rmse:71953.9\u001b[0m\n",
      "\u001b[34m[64]#011train-rmse:71653.5\u001b[0m\n",
      "\u001b[34m[65]#011train-rmse:71455.2\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:71162.5\u001b[0m\n",
      "\u001b[34m[67]#011train-rmse:70892.8\u001b[0m\n",
      "\u001b[34m[68]#011train-rmse:70782.7\u001b[0m\n",
      "\u001b[34m[69]#011train-rmse:70660.3\u001b[0m\n",
      "\u001b[34m[70]#011train-rmse:70421\u001b[0m\n",
      "\u001b[34m[71]#011train-rmse:70252.4\u001b[0m\n",
      "\u001b[34m[72]#011train-rmse:70116.1\u001b[0m\n",
      "\u001b[34m[73]#011train-rmse:69944.8\u001b[0m\n",
      "\u001b[34m[74]#011train-rmse:69672.4\u001b[0m\n",
      "\u001b[34m[75]#011train-rmse:69379.5\u001b[0m\n",
      "\u001b[34m[76]#011train-rmse:68847.9\u001b[0m\n",
      "\u001b[34m[77]#011train-rmse:68483.3\u001b[0m\n",
      "\u001b[34m[78]#011train-rmse:68216.7\u001b[0m\n",
      "\u001b[34m[79]#011train-rmse:67982.3\u001b[0m\n",
      "\u001b[34m[80]#011train-rmse:67780.5\u001b[0m\n",
      "\u001b[34m[81]#011train-rmse:67511.1\u001b[0m\n",
      "\u001b[34m[82]#011train-rmse:67142.4\u001b[0m\n",
      "\u001b[34m[83]#011train-rmse:66849.3\u001b[0m\n",
      "\u001b[34m[84]#011train-rmse:66698.1\u001b[0m\n",
      "\u001b[34m[85]#011train-rmse:66556.6\u001b[0m\n",
      "\u001b[34m[86]#011train-rmse:66242.1\u001b[0m\n",
      "\u001b[34m[87]#011train-rmse:65808.6\u001b[0m\n",
      "\u001b[34m[88]#011train-rmse:65589\u001b[0m\n",
      "\u001b[34m[89]#011train-rmse:65334.7\u001b[0m\n",
      "\u001b[34m[90]#011train-rmse:65109\u001b[0m\n",
      "\u001b[34m[91]#011train-rmse:64791.5\u001b[0m\n",
      "\u001b[34m[92]#011train-rmse:64642.5\u001b[0m\n",
      "\u001b[34m[93]#011train-rmse:64472.6\u001b[0m\n",
      "\u001b[34m[94]#011train-rmse:64206.7\u001b[0m\n",
      "\u001b[34m[95]#011train-rmse:64102.2\u001b[0m\n",
      "\u001b[34m[96]#011train-rmse:63914.4\u001b[0m\n",
      "\u001b[34m[97]#011train-rmse:63698.6\u001b[0m\n",
      "\u001b[34m[98]#011train-rmse:63408.9\u001b[0m\n",
      "\u001b[34m[99]#011train-rmse:63258.7\u001b[0m\n",
      "\n",
      "2020-03-14 00:46:48 Uploading - Uploading generated training model\n",
      "2020-03-14 00:46:48 Completed - Training job completed\n",
      "Training seconds: 70\n",
      "Billable seconds: 70\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy our model. This will create an endpoint holding our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = xgb.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model deployed, we can use it to make predictions.\n",
    "\n",
    "Notice that it does take a little bit of work to ensure that we are sending our data to the model in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.content_type = 'text/csv'\n",
    "predictor.serializer = csv_serializer\n",
    "predictor.deserializer = None\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(test.values[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71036.91989792968"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(test.iloc[:,0], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: if you are making a large number of predictions, you will have to do it in batches, because is a cap on how much data you can pass to an endpoint at a time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
